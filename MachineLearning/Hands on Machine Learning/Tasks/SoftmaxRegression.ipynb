{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".box {\n",
       "    border: 1px double dodgerblue;\n",
       "    padding: 5px;\n",
       "}\n",
       ".note {\n",
       "    font-size: 20;\n",
       "    color: teal;\n",
       "    font-weight: bold;\n",
       "}\n",
       ".highlight {\n",
       "    color: green;\n",
       "    font-family: verdana;\n",
       "}\n",
       ".tag {\n",
       "    background: red;\n",
       "    color: white;\n",
       "    padding: 3px;\n",
       "}\n",
       ".warning {\n",
       "    background: red;\n",
       "    padding: 12px;\n",
       "    font-size: 24px;\n",
       "    color: white;\n",
       "    text-align: center;\n",
       "    font-family: verdana;\n",
       "}\n",
       ".symbol {\n",
       "    color: white;\n",
       "    background: green;\n",
       "    font-size: 20px;\n",
       "    padding: 2px;\n",
       "}\n",
       "code {\n",
       "    font-weight: bold;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%run ..\\convention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Batch Gradient Descent with early stopping for Softmax Regression\n",
    "(without using Scikit-Learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "def softmax(arr):\n",
    "    res = np.exp(arr - arr.max(axis = 1, keepdims = True))\n",
    "    return res / res.sum(axis = 1, keepdims = True)\n",
    "\n",
    "class SoftmaxRegression(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, alpha = 0, n_iter = 200, warn_start = False, RandomState = 0, learning_rate = .001):\n",
    "        self.alpha = alpha\n",
    "        self.warn_start = warn_start\n",
    "        self.RandomState = RandomState\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.coefs_ = None\n",
    "        self.intercept_ = None\n",
    "    def fit(self, X, y):\n",
    "        M, N = X.shape\n",
    "        C = len(np.unique(y))\n",
    "        y = pd.get_dummies(y).values\n",
    "        \n",
    "        if self.warn_start == False or self.coefs_ is None:\n",
    "            r = np.random.RandomState(self.RandomState)\n",
    "            self.coefs_ = np.random.rand(C, N)\n",
    "            self.intercept_ = np.random.rand(C)\n",
    "        \n",
    "        W, b = self.coefs_, self.intercept_\n",
    "        for _ in range(self.n_iter):\n",
    "            z = X @ W.T + b\n",
    "            a = softmax(z)\n",
    "            error = a - y\n",
    "            dW = np.array([np.sum(X * error[:, [i]], axis = 0) for i in range(C)])\n",
    "            b -= self.learning_rate * (self.alpha * b + error.sum(0))\n",
    "            W -= self.learning_rate * (self.alpha * W + dW)\n",
    "        \n",
    "        self.coefs_ = W\n",
    "        self.intercept_ = b\n",
    "        self.cost = -np.sum(y * np.log(a + 1e-10)) #cross-entropy\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return softmax(X @ self.coefs_.T + self.intercept_).argmax(axis = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116.01424929841798 | 0.7894736842105263\n",
      "144.78743791474085 | 0.8947368421052632\n",
      "38.03764969782503 | 0.9736842105263158\n",
      "20.296143984362466 | 1.0\n",
      "13.259509375107395 | 1.0\n",
      "11.769200722462479 | 1.0\n",
      "11.638915411681355 | 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "clf = SoftmaxRegression(warn_start = True, n_iter = 1, learning_rate = 0.1)\n",
    "prev_test_score = 0\n",
    "while True:\n",
    "    best_coefs_ = clf.coefs_\n",
    "    best_intercept_ = clf.intercept_\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cur_test_score = accuracy_score(y_pred, y_test)\n",
    "    print(clf.cost, '|', cur_test_score)    \n",
    "    if cur_test_score < prev_test_score: break\n",
    "    prev_test_score = cur_test_score\n",
    "\n",
    "clf.coefs_ = best_coefs_\n",
    "clf.intercept_ = best_intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.97\n",
      "Train score: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Test score: %.2f' % accuracy_score(y_pred, y_test))\n",
    "print('Train score: %.2f' % accuracy_score(clf.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit learning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#by default, LogisticRegression is implemented in skclearn by One-vs-All method\n",
    "#to use Softmax regression, set the keyword multi_class = 'multinomial'\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', solver = 'lbfgs', C = 10)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train score: 1.0'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Train score: {}'.format(clf.score(X_train, clf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test score: 1.0'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Test score: {}'.format(clf.score(X_test, clf.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
