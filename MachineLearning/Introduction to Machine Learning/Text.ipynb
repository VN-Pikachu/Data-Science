{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".box {\n",
       "    border: 1px double dodgerblue;\n",
       "    padding: 5px;\n",
       "}\n",
       ".note {\n",
       "    font-size: 20;\n",
       "    color: teal;\n",
       "    font-weight: bold;\n",
       "}\n",
       ".highlight {\n",
       "    color: green;\n",
       "    font-family: verdana;\n",
       "}\n",
       ".tag {\n",
       "    background: red;\n",
       "    color: white;\n",
       "    padding: 3px;\n",
       "}\n",
       ".warning {\n",
       "    background: red;\n",
       "    padding: 12px;\n",
       "    font-size: 24px;\n",
       "    color: white;\n",
       "    text-align: center;\n",
       "    font-family: verdana;\n",
       "}\n",
       ".symbol {\n",
       "    color: white;\n",
       "    background: green;\n",
       "    font-size: 20px;\n",
       "    padding: 2px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%run convention.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Data Represented as Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Categorical data</li>\n",
    "<li>Free strings that can be semantically mapped to categories</li>\n",
    "<li>Structured string data</li>\n",
    "<li>Text data</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Text Data as a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'note'> Only count how often each word appears in each text in the corpus.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the bag-of-words representation for a corpus of documents consists of\n",
    "the following three steps:\n",
    "<ol>\n",
    "<li><i>Tokenization</i>. Split each document into the words that appear in it (called tokens),\n",
    "for example by splitting them on whitespace and punctuation.\n",
    "</li>\n",
    "<li>\n",
    "<i>Vocabulary building</i>. Collect a vocabulary of all words that appear in any of the\n",
    "documents, and number them (say, in alphabetical order).\n",
    "</li>\n",
    "<li>\n",
    "<i>Encoding</i>. For each document, count how often each of the words in the vocabu‚Äê\n",
    "lary appear in this document.\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bag-of-Words to a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric = ['An empty street', 'An empty house']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x4 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "vec.fit(lyric)\n",
    "bow = vec.transform(lyric)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'empty', 'house', 'street']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>empty</th>\n",
       "      <th>house</th>\n",
       "      <th>street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  empty  house  street\n",
       "0   1      1      0       1\n",
       "1   1      1      1       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(bow.toarray(), columns = vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.92\n",
      "Test score: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "groups = fetch_20newsgroups()\n",
    "X, y = CountVectorizer().fit_transform(groups.data), groups.target \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 6)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print('Train score: %.2f' % clf.score(X_train, y_train))\n",
    "print('Test score: %.2f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that <code>MultinomialNB</code> has a regularization parameter <code>alpha</code>, which can be tuned via cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best validation score: 0.87'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_params = {'alpha' : [.001, .01, .1, 1, 10, 100]}\n",
    "grid = GridSearchCV(MultinomialNB(), grid_params, cv = 5)\n",
    "grid.fit(X_train, y_train)\n",
    "'Best validation score: %.2f' % grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test score: 0.87'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Test score: %.2f' % grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'note'>A token that appears only in a single document is unlikely to appear in the test\n",
    "set and is therefore not helpful. We can set the minimum number of documents a\n",
    "token needs to appear in with the min_df parameter:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(min_df = 2) #select tokens that appear in at least 2 documents\n",
    "corpus = [' I Love You',\n",
    "          ' You are my heart, you are my soul',\n",
    "          'Love the way you lie']\n",
    "vec.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>love</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   love  you\n",
       "0     1    1\n",
       "1     0    2\n",
       "2     1    1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(vec.transform(corpus).toarray(), columns = vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'note'>Another way that we can get rid of uninformative words is by discarding words that\n",
    "are too frequent to be informative</p>\n",
    "<ul>There are 2 strategies:\n",
    "    <li> using a languagespecific list of stopwords</li>\n",
    "    <li>iscarding words that appear too frequently</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'highlight'>scikitlearn has a built-in list of English stopwords in the feature_extraction.text\n",
    "module:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'note'>Clearly, removing the stopwords in the list can only decrease the number of features\n",
    "by the length of the list but it might lead to an improvement in performance.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try to remove stop words to see if we can improve our model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87982456, 0.87609842, 0.86906854, 0.8943662 , 0.88162544,\n",
       "       0.8591674 , 0.88120567, 0.87833037, 0.88434164, 0.88512912])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "vec = CountVectorizer(min_df = 5, stop_words='english')\n",
    "X, y = vec.fit_transform(groups.data), groups.target\n",
    "clf = MultinomialNB(alpha = .001)\n",
    "scores = cross_val_score(clf, X, y, cv = 10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8789157366848009"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'note'>Discarding words that appear frequently: set the keyword: <code>max_df</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' I Love You', ' You are my heart, you are my soul', 'Love the way you lie']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are', 'heart', 'lie', 'love', 'my', 'soul', 'the', 'way']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(max_df = 2) #discard words that appear >= 2 times\n",
    "vec.fit(corpus)\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recaling data with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'note'>\n",
    "    Instead of dropping features that are deemed unimportant, another approach is to\n",
    "rescale features by how informative we expect them to be. One of the most common\n",
    "    ways to do this is using the term  <b>frequency‚Äìinverse document frequency (tf‚Äìidf)</b>\n",
    "method. The intuition of this method is to give high weight to any term that appears\n",
    "often in a particular document, but not in many documents in the corpus. If a word\n",
    "appears often in a particular document, but not in very many documents, it is likely\n",
    "to be very descriptive of the content of that document. scikit-learn implements the\n",
    "    tf‚Äìidf method in two classes: <code>TfidfTransformer</code>, which takes in the sparse matrix\n",
    "    output produced by <code>CountVectorizer</code> and transforms it, and <code>TfidfVectorizer</code>,\n",
    "which takes in the text data and does both the bag-of-words feature extraction and\n",
    "the tf‚Äìidf transformation. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'warning'>\n",
    "Because tf‚Äìidf actually makes use of the statistical properties of the training data, we\n",
    "will use a pipeline, to ensure the results of our grid search\n",
    "are valid\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best cross-validation score: 0.91'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "grid_params = {'multinomialnb__alpha' : [.0001, .001, .01, .1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, grid_params, cv = 5)\n",
    "grid.fit(groups.data, groups.target)\n",
    "'Best cross-validation score: %.2f' % grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class = 'highlight'>As you can see, there is some improvement when using tf‚Äìidf instead of just word\n",
    "counts. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect which words tf‚Äìidf found most important: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the TfidfTvectorizer class\n",
    "vec = grid.best_estimator_.named_steps['tfidfvectorizer']\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vector = vec.transform(groups.data)\n",
    "importance = corpus_vector.max(axis = 0).toarray().ravel()\n",
    "arg = np.argsort(importance)\n",
    "\n",
    "order = np.array(vec.get_feature_names())[arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"features with highest tfidf: ['kk' 'db' 'scsi' 'blah' 'donoghue' '00' '___' '25' 'forged' 'ax']\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'features with highest tfidf: {}'.format(order[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"features with lowest tfidf: ['giz1z4' 'newwhjnux' 'newwhj' 'ne1wj' 'ne1whj' 'biz1pmf' 'mzwt' '9j6e1t'\\n '9tad9' '9tagm']\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'features with lowest tfidf: {}'.format(order[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad-of-words with more than one word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main disadvantages of using a bag-of-words representation is that word\n",
    "order is completely discarded. Therefore, the two strings ‚Äúit‚Äôs bad, not good at all‚Äù and\n",
    "‚Äúit‚Äôs good, not bad at all‚Äù have exactly the same representation, even though the mean‚Äê\n",
    "ings are inverted. Putting ‚Äúnot‚Äù in front of a word is only one example (if an extreme\n",
    "one) of how context matters. Formtunately, there is a way of capturing context when\n",
    "using a bag-of-words representation, by not only considering the counts of single\n",
    "tokens, but also the counts of pairs or triplets of tokens that appear next to each other.\n",
    "Pairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\n",
    "more generally sequences of tokens are known as n-grams. We can change the range\n",
    "of tokens that are considered as features by changing the <code>ngram_range</code> parameter of\n",
    "<code>CountVectorizer</code> or <code>TfidfVectorizer</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at all',\n",
       " 'bad at',\n",
       " 'bad not',\n",
       " 'good at',\n",
       " 'good not',\n",
       " 'it bad',\n",
       " 'it good',\n",
       " 'not bad',\n",
       " 'not good']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"It's good, not bad at all!\", \"It's bad, not good at all\"]\n",
    "vec = TfidfVectorizer(ngram_range = (2,2)) #select 2 consecutive words at a time\n",
    "vec.fit(corpus)\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'at',\n",
       " 'at all',\n",
       " 'bad',\n",
       " 'bad at',\n",
       " 'bad not',\n",
       " 'good',\n",
       " 'good at',\n",
       " 'good not',\n",
       " 'it',\n",
       " 'it bad',\n",
       " 'it good',\n",
       " 'not',\n",
       " 'not bad',\n",
       " 'not good']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range = (1,2)) #select 1 word at a time, then 2 words at a time\n",
    "vec.fit(corpus)\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class = 'warning'>NOTE:</span>\n",
    "<p class = 'highlight'>\n",
    "For most applications, the minimum number of tokens should be one, as single\n",
    "words often capture a lot of meaning. Adding bigrams helps in most cases. Adding\n",
    "longer sequences‚Äîup to 5-grams‚Äîmight help too, but this will lead to an explosion\n",
    "of the number of features and might lead to overfitting, as there will be many very\n",
    "specific features. In principle, the number of bigrams could be the number of\n",
    "unigrams squared and the number of trigrams could be the number of unigrams to\n",
    "the power of three, leading to very large feature spaces. In practice, the number of\n",
    "higher n-grams that actually appear in the data is much smaller, because of the struc‚Äê\n",
    "ture of the (English) language, though it is still large\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tfidfvectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=5,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True...\n",
       "                                                        tokenizer=None,\n",
       "                                                        use_idf=True,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('multinomialnb',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'multinomialnb__alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'tfidfvectorizer__ngram_range': [(1, 1), (1, 2),\n",
       "                                                          (1, 3)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using bigram to test whether we can imporve our model's performance or not\n",
    "vec = TfidfVectorizer(min_df = 5, stop_words = 'english')\n",
    "pipe = make_pipeline(vec, MultinomialNB())\n",
    "grid_params = {\n",
    "    'tfidfvectorizer__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "    'multinomialnb__alpha' : [.001, .01, .1, 1, 10, 100]\n",
    "}\n",
    "grid = GridSearchCV(pipe, grid_params, cv = 5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(groups.data, groups.target, random_state = 6)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multinomialnb__alpha': 0.1, 'tfidfvectorizer__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best cross-validation-score: 0.89'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Best cross-validation-score: %.2f' % grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tokenization, Stemming, and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, the feature extraction in the CountVectorizer and Tfidf\n",
    "Vectorizer is relatively simple, and much more elaborate methods are possible. One\n",
    "particular step that is often improved in more sophisticated text-processing applica‚Äê\n",
    "tions is the first step in the bag-of-words model: tokenization. This step defines what\n",
    "constitutes a word for the purpose of feature extraction.\n",
    "We saw earlier that the vocabulary often contains singular and plural versions of\n",
    "some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n",
    "\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\n",
    "of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\n",
    "increase overfitting, and not allow the model to fully exploit the training data. Simi‚Äê\n",
    "larly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\n",
    "ment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\n",
    "relating to the verb ‚Äúto replace.‚Äù Similarly to having singular and plural forms of a\n",
    "noun, treating different verb forms and related words as distinct tokens is disadvanta‚Äê\n",
    "geous for building a model that generalizes well.\n",
    "This problem can be overcome by representing each word using its word stem, which\n",
    "involves identifying (or conflating) all the words that have the same word stem. If this\n",
    "is done by using a rule-based heuristic, like dropping common suffixes, it is usually\n",
    "referred to as stemming. If instead a dictionary of known word forms is used (an\n",
    "explicit and human-verified system), and the role of the word in the sentence is taken\n",
    "into account, the process is referred to as lemmatization and the standardized form of\n",
    "the word is referred to as the lemma. Both processing methods, lemmatization and\n",
    "stemming, are forms of normalization that try to extract some normal form of a\n",
    "word. Another interesting case of normalization is spelling correction, which can be\n",
    "helpful in practice but is outside of the scope of this book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling and Document Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
